{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f19e4074",
   "metadata": {},
   "source": [
    "# Finding Labels\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "We've thousands of app reviews for any given client. We do not know what categories or tags would apply. The intent behind this notebook is to explore one approach, which can combine data with your understanding of the data/business context. \n",
    "\n",
    "## Approach\n",
    "\n",
    "Step 1: LDA to form topics and classify each document into a topic\n",
    "\n",
    "Step 2: \n",
    "\n",
    "a) Find linguistic terms e.g. noun chunks, entities, n-grams for each topic\n",
    "\n",
    "b) Using YAKE, an unsupervised Keyword extractor to find more keywords for each topic \n",
    "\n",
    "Step 3: Manually read the top few of both LDA and combined list from Step 2 to select labels which are of interest to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc4a369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import textacy\n",
    "from pydantic import BaseModel\n",
    "from spacy.lang.en import English\n",
    "from textacy import extract\n",
    "from textacy.representations.vectorizers import Vectorizer\n",
    "from textacy.similarity.tokens import jaccard\n",
    "from textacy.tm import TopicModel\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9364bba0",
   "metadata": {},
   "source": [
    "The main contribution here is a programmatic way to find labels for topic models, and then classify documents into them -- but while still retaining some degree of human intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2adc034e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/nirantk/Documents/GitHub/AppReview/data/raw/Clubhouse_us_app_store_reviews.json'),\n",
       " WindowsPath('C:/Users/nirantk/Documents/GitHub/AppReview/data/raw/com.ubercab_us_play_store_reviews.json'),\n",
       " WindowsPath('C:/Users/nirantk/Documents/GitHub/AppReview/data/raw/frequency_count.json'),\n",
       " WindowsPath('C:/Users/nirantk/Documents/GitHub/AppReview/data/raw/IndiaGold_in_app_store_reviews.json'),\n",
       " WindowsPath('C:/Users/nirantk/Documents/GitHub/AppReview/data/raw/Moj_in_app_store_reviews.json'),\n",
       " WindowsPath('C:/Users/nirantk/Documents/GitHub/AppReview/data/raw/Moj_us_app_store_reviews.json'),\n",
       " WindowsPath('C:/Users/nirantk/Documents/GitHub/AppReview/data/raw/Netflix_us_app_store_reviews.json'),\n",
       " WindowsPath('C:/Users/nirantk/Documents/GitHub/AppReview/data/raw/Uber_us_app_store_reviews.json')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = Path(\"../data/raw\").resolve()\n",
    "assert data_dir.exists()\n",
    "data_dir.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2d73f",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "Here we will explore App Reviews for just one app: Uber (Passenger/Cab, not the Driver). The additional data to reproduce this for other clients is left as an exercise for you.\n",
    "\n",
    "But to get an overview of all of them, we combine them into a larger single text string and explore them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb4aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = data_dir / \"Uber_us_app_store_reviews.json\"; assert file_path.exists()\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = pd.read_json(f)\n",
    "    reviews = raw_data[\"review\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de20c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load(\"en_core_web_trf\") # for higher accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c35a824b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time reviews = [rev.strip() for rev in reviews]\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d0e0d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 59s\n"
     ]
    }
   ],
   "source": [
    "%time corpus = textacy.Corpus(\"en_core_web_sm\", data=reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f68e6443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 43269, 729300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.n_docs, corpus.n_sents, corpus.n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5aab5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('driver', 8032),\n",
       " ('Uber', 7978),\n",
       " ('ride', 4797),\n",
       " ('time', 4369),\n",
       " ('app', 4232),\n",
       " ('charge', 2756),\n",
       " ('uber', 2527),\n",
       " ('minute', 2192),\n",
       " ('cancel', 2157),\n",
       " ('service', 2135),\n",
       " ('use', 2080),\n",
       " ('pick', 1940),\n",
       " ('wait', 1928),\n",
       " ('customer', 1916),\n",
       " ('$', 1834),\n",
       " ('car', 1791),\n",
       " ('try', 1774),\n",
       " ('go', 1639),\n",
       " ('get', 1607),\n",
       " ('way', 1586),\n",
       " ('work', 1516),\n",
       " ('trip', 1514),\n",
       " ('say', 1513),\n",
       " ('take', 1460),\n",
       " ('need', 1438)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = corpus.word_counts(by=\"lemma_\", filter_stops= True, filter_nums=True, filter_punct=True)\n",
    "sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4daa05",
   "metadata": {},
   "source": [
    "## Topic Modeling with Textacy via Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15d44e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = (\n",
    "    (term.lemma_ for term in textacy.extract.terms(doc, ngs=1, ents=True))\n",
    "    for doc in corpus\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0d88166",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer(\n",
    "    tf_type=\"linear\", idf_type=\"smooth\", norm=\"l2\", min_df=3, max_df=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b27d978a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x4544 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 203279 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix = vectorizer.fit_transform(tokenized_docs)\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f7f72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a75dee3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nirantk\\miniconda3\\envs\\book\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:315: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  \"'nndsvda' in 1.1 (renaming of 0.26).\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = TopicModel(\"nmf\", n_topics=n_topics)\n",
    "model.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "919078d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix = model.transform(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52d7eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topic(BaseModel):\n",
    "    title: Optional[str]\n",
    "    terms: Optional[List[str]]\n",
    "    top_docs_idx: Optional[List[int]]\n",
    "    keyterms: Optional[Dict] = {}\n",
    "    linguistic_terms: Optional[Dict] = {}\n",
    "\n",
    "lst_topics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73fd9e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: app\tUber\tdriver\tride\ttime\twork\tuse\tneed\tlike\tgreat\n",
      "Topic #1: customer\tservice\tissue\tsupport\tcontact\tresponse\tUber\tcompany\tresolve\trefund\n",
      "Topic #2: cancel\tdriver\tcharge\tfee\twait\ttrip\tcancellation\tpick\tcall\trequest\n",
      "Topic #3: walk\tdrop\tpick\tlocation\tpool\tblock\tdestination\taddress\tpoint\tstreet\n",
      "Topic #4: $\tcharge\tprice\tride\tUber\tpay\tcost\tmoney\tdollar\ttake\n",
      "Topic #5: card\tcredit\tpayment\tgift\tmethod\tcash\tuse\tUber\taccount\tdebit\n",
      "Topic #6: minute\twait\taway\ttime\t10\t20\t10 minute\t5\t15\tlate\n",
      "Topic #7: uber\tlyft\tmoney\ttime\tuse\tnt\twork\tneed\tlike\tbad\n",
      "Topic #8: car\tdrive\tseat\tget\tairport\task\tsay\tclean\tsmell\tdriver\n",
      "Topic #9: account\tnumber\tphone\temail\ttry\thelp\tlog\tsign\tapp\tUber\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, top_terms in model.top_topic_terms(\n",
    "    vectorizer.id_to_term, topics=range(n_topics)\n",
    "):\n",
    "    print(f\"Topic #{topic_idx}:\", \"\\t\".join(top_terms))\n",
    "    lst_topics.append(Topic(terms=[str(term) for term in top_terms]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5ca5778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top documents for each topic\n",
    "for topic_idx, top_docs_idx in model.top_topic_docs(\n",
    "    doc_topic_matrix, weights=True, top_n=20\n",
    "):\n",
    "    #     print(f\"{topic_idx}: {top_docs_idx}\")\n",
    "    lst_topics[topic_idx].top_docs_idx = top_docs_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7b9d15",
   "metadata": {},
   "source": [
    "# Exploring Each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90fdcc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms(terms: List[Tuple[str, int]]) -> List[str]:\n",
    "    return [ele[0] for ele in terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2298001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Ngrams, Entities and Noun Chunks for each document in a topic\n",
    "for topic in lst_topics:\n",
    "    l_terms, k_terms = [], []\n",
    "    for doc_idx, weight in topic.top_docs_idx:\n",
    "        doc = corpus[doc_idx]\n",
    "        l_terms.extend(\n",
    "            [\n",
    "                str(term).lower()  # ignore case\n",
    "                for term in extract.terms(doc, ngs=[2, 3], ncs=True, ents=True)\n",
    "            ]\n",
    "        )\n",
    "        k_terms.extend([pair[0] for pair in extract.keyterms.yake(doc, ngrams=[2, 3])])\n",
    "    #     print(len(topic.linguistic_terms))\n",
    "    topic.linguistic_terms = Counter(l_terms).most_common(200)\n",
    "    topic.keyterms = Counter(k_terms).most_common(200)\n",
    "    topic.linguistic_terms, topic.keyterms = get_terms(\n",
    "        topic.linguistic_terms\n",
    "    ), get_terms(topic.keyterms)\n",
    "    # remove duplicates across docs\n",
    "#     print(len(topic.linguistic_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7023e113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: ['app', 'driver', 'ride', 'time', 'work', 'need', 'helpful driver', 'great rating', 'cancellation tab', 'star rating', 'bad review', 'good driver', 'speed limit', 'Uber', 'use', 'like', 'great', 'Lyft app', 'couple time', 'driver time']\n",
      "LDA Terms: ['app', 'Uber', 'driver', 'ride', 'time', 'work', 'use', 'need', 'like', 'great']\n",
      "-------------------------------------------------------------\n",
      "Topic 2: ['customer', 'service', 'issue', 'support', 'response', 'company', 'refund', 'customer service', 'customer support', 'service number', 'customer service number', 'phone number', 'terrible customer', 'poor customer', 'poor customer service', 'horrible customer', 'horrible customer service', 'good luck', 'service assistance', 'emergency situation']\n",
      "LDA Terms: ['customer', 'service', 'issue', 'support', 'contact', 'response', 'Uber', 'company', 'resolve', 'refund']\n",
      "-------------------------------------------------------------\n",
      "Topic 3: ['driver', 'fee', 'trip', 'request', 'cancellation fee', 'new driver', 'phone number', 'second driver', 'long time', 'cancel fee', 'new ride', 'credit card', 'uber driver', 'cancel', 'charge', 'wait', 'cancellation', 'pick', 'call', 'Uber driver']\n",
      "LDA Terms: ['cancel', 'driver', 'charge', 'fee', 'wait', 'trip', 'cancellation', 'pick', 'call', 'request']\n",
      "-------------------------------------------------------------\n",
      "Topic 4: ['drop', 'pick', 'location', 'pool', 'block', 'destination', 'address', 'point', 'street', 'uber pool', 'new pool', 'different location', 'recent update', 'min walk', 'place close', 'specific drop', 'exact location', 'block walk', 'original request', 'app wrong']\n",
      "LDA Terms: ['walk', 'drop', 'pick', 'location', 'pool', 'block', 'destination', 'address', 'point', 'street']\n",
      "-------------------------------------------------------------\n",
      "Topic 5: ['price', 'ride', 'cost', 'money', 'dollar ride', 'customer service', 'basic service', 'monthly metrocard', 'minute ride', 'uber cash', 'asking price', 'car service', 'customer support', 'awful user', 'user experience', 'awful user experience', 'ride pass', '$', 'charge', 'Uber']\n",
      "LDA Terms: ['$', 'charge', 'price', 'ride', 'Uber', 'pay', 'cost', 'money', 'dollar', 'take']\n",
      "-------------------------------------------------------------\n",
      "Topic 6: ['card', 'payment', 'use', 'account', 'credit card', 'gift card', 'payment method', 'debit card', 'card balance', 'gift card balance', 'card info', 'personal card', 'card information', 'uber gift', 'new account', 'customer service', 'payment info', 'gift card info', 'credit card info', 'business credit']\n",
      "LDA Terms: ['card', 'credit', 'payment', 'gift', 'method', 'cash', 'use', 'Uber', 'account', 'debit']\n",
      "-------------------------------------------------------------\n",
      "Topic 7: ['minute', 'time', '10', 'wait time', 'minute wait', 'arrival time', 'second driver', 'affordable option', 'gas station', 'drunk driving', 'minute delay', 'drive cancel', 'accurate wait', 'ride pass', 'wait', 'away', '20', '10 minute', '5', '15']\n",
      "LDA Terms: ['minute', 'wait', 'away', 'time', '10', '20', '10 minute', '5', '15', 'late']\n",
      "-------------------------------------------------------------\n",
      "Topic 8: ['uber', 'lyft', 'money', 'time', 'work', 'uber driver', 'customer service', 'uber ride', 'extra charge', 'charge fee', 'new uber', 'extra charge fee', 'timely fashion', 'regular basis', 'regular customer', 'time request', 'uber way', 'report multiple', 'fee money', 'drunk uber']\n",
      "LDA Terms: ['uber', 'lyft', 'money', 'time', 'use', 'nt', 'work', 'need', 'like', 'bad']\n",
      "-------------------------------------------------------------\n",
      "Topic 9: ['car', 'airport', 'smell', 'driver', 'car seat', 'correct car', 'car color', 'right car', 'car ride', 'nice woman', 'safe drive', 'wonderful employee', 'specific car', 'specific car ride', 'blue car', 'grey car', 'family member', 'upcoming traffic', 'correct car color', 'past week']\n",
      "LDA Terms: ['car', 'drive', 'seat', 'get', 'airport', 'ask', 'say', 'clean', 'smell', 'driver']\n",
      "-------------------------------------------------------------\n",
      "Topic 10: ['account', 'number', 'email', 'try', 'help', 'app', 'phone number', 'new account', 'email address', 'new number', 'customer service', 'old number', 'credit card', 'new phone', 'service number', 'old account', 'customer support', 'old phone', 'old phone number', 'uber support']\n",
      "LDA Terms: ['account', 'number', 'phone', 'email', 'try', 'help', 'log', 'sign', 'app', 'Uber']\n",
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Finding a Title for Each Topic\n",
    "for i, topic in enumerate(lst_topics):\n",
    "    combined_list = topic.terms + topic.keyterms + topic.linguistic_terms\n",
    "    combined_list = Counter(combined_list).most_common(20)\n",
    "    print(f\"Topic {i+1}:\", get_terms(combined_list))\n",
    "    print(f\"LDA Terms: {topic.terms}\")\n",
    "    print(\"-------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd0e4f",
   "metadata": {},
   "source": [
    "The above list of terms for each topic give us an indication in addtion to LDA terms of what each topic is about. At this point, I read the abover terms and select the following labels to be of interest to me. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1338a",
   "metadata": {},
   "source": [
    "# Labels\n",
    "\n",
    "Happy with the app [Skipped]\n",
    "\n",
    "|Index|Label Title| Label Description|\n",
    "|---|:---|:---|\n",
    "|1|payment|Payment Methods|\n",
    "|2|cancel_fees|Cancellation Fee|\n",
    "|3|price|Price|\n",
    "|4|pickup|Pickup|\n",
    "|5|pool|Pool|\n",
    "|6|support|Customer Support|\n",
    "|7|advance_ride|Advance Ride Booking|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
