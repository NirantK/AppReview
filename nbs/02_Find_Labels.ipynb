{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdefc109",
   "metadata": {},
   "source": [
    "# Finding Labels\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "We've thousands of app reviews for any given client. We do not know what categories or tags would apply. The intent behind this notebook is to explore one approach, which can combine data with your understanding of the data/business context. \n",
    "\n",
    "## Approach\n",
    "\n",
    "Step 1: We use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc4a369a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import textacy\n",
    "from pydantic import BaseModel\n",
    "from spacy.lang.en import English\n",
    "from textacy import extract\n",
    "from textacy.representations.vectorizers import Vectorizer\n",
    "from textacy.similarity.tokens import jaccard\n",
    "from textacy.tm import TopicModel\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9364bba0",
   "metadata": {},
   "source": [
    "The main contribution here is a programmatic way to find labels for topic models, and then classify documents into them -- but while still retaining some degree of human intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2adc034e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/nirantk/Documents/GitHub/AppReview/data/raw/Clubhouse_us_app_store_reviews.json'),\n",
       " WindowsPath('C:/Users/nirantk/Documents/GitHub/AppReview/data/raw/com.ubercab_us_play_store_reviews.json'),\n",
       " WindowsPath('C:/Users/nirantk/Documents/GitHub/AppReview/data/raw/frequency_count.json'),\n",
       " WindowsPath('C:/Users/nirantk/Documents/GitHub/AppReview/data/raw/IndiaGold_in_app_store_reviews.json'),\n",
       " WindowsPath('C:/Users/nirantk/Documents/GitHub/AppReview/data/raw/Moj_in_app_store_reviews.json'),\n",
       " WindowsPath('C:/Users/nirantk/Documents/GitHub/AppReview/data/raw/Moj_us_app_store_reviews.json'),\n",
       " WindowsPath('C:/Users/nirantk/Documents/GitHub/AppReview/data/raw/Netflix_us_app_store_reviews.json'),\n",
       " WindowsPath('C:/Users/nirantk/Documents/GitHub/AppReview/data/raw/Uber_us_app_store_reviews.json')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = Path(\"../data/raw\").resolve()\n",
    "assert data_dir.exists()\n",
    "data_dir.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2d73f",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "Here we will explore App Reviews for just one app: Uber (Passenger/Cab, not the Driver). The additional data to reproduce this for other clients is left as an exercise for you.\n",
    "\n",
    "But to get an overview of all of them, we combine them into a larger single text string and explore them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebb4aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = data_dir / \"Uber_us_app_store_reviews.json\"; assert file_path.exists()\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = pd.read_json(f)\n",
    "    reviews = raw_data[\"review\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de20c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c35a824b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 987 Âµs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time reviews = [rev.strip() for rev in reviews]\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d0e0d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 14s\n"
     ]
    }
   ],
   "source": [
    "%time corpus = textacy.Corpus(\"en_core_web_sm\", data=reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f68e6443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 43269, 729300)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.n_docs, corpus.n_sents, corpus.n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5aab5ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('driver', 8032),\n",
       " ('Uber', 7978),\n",
       " ('ride', 4797),\n",
       " ('time', 4369),\n",
       " ('app', 4232),\n",
       " ('charge', 2756),\n",
       " ('uber', 2527),\n",
       " ('minute', 2192),\n",
       " ('cancel', 2157),\n",
       " ('service', 2135),\n",
       " ('use', 2080),\n",
       " ('pick', 1940),\n",
       " ('wait', 1928),\n",
       " ('customer', 1916),\n",
       " ('$', 1834),\n",
       " ('car', 1791),\n",
       " ('try', 1774),\n",
       " ('go', 1639),\n",
       " ('get', 1607),\n",
       " ('way', 1586),\n",
       " ('work', 1516),\n",
       " ('trip', 1514),\n",
       " ('say', 1513),\n",
       " ('take', 1460),\n",
       " ('need', 1438)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = corpus.word_counts(by=\"lemma_\", filter_stops= True, filter_nums=True, filter_punct=True)\n",
    "sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d669fb39",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- [ ] NMF Topic Modeling\n",
    "- [ ] Noun Chunks\n",
    "- [ ] N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4daa05",
   "metadata": {},
   "source": [
    "## Topic Modeling with Textacy via Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15d44e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = (\n",
    "    (term.lemma_ for term in textacy.extract.terms(doc, ngs=1, ents=True))\n",
    "    for doc in corpus\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0d88166",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer(\n",
    "    tf_type=\"linear\", idf_type=\"smooth\", norm=\"l2\", min_df=3, max_df=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b27d978a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5000x4544 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 203279 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix = vectorizer.fit_transform(tokenized_docs)\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f7f72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a75dee3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nirantk\\miniconda3\\envs\\book\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:315: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  \"'nndsvda' in 1.1 (renaming of 0.26).\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = TopicModel(\"nmf\", n_topics=n_topics)\n",
    "model.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "919078d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix = model.transform(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52d7eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topic(BaseModel):\n",
    "    title: Optional[str]\n",
    "    terms: Optional[List[str]]\n",
    "    top_docs_idx: Optional[List[int]]\n",
    "    keyterms: Optional[Dict] = {}\n",
    "    linguistic_terms: Optional[Dict] = {}\n",
    "\n",
    "lst_topics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73fd9e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: app\tUber\tdriver\tride\ttime\twork\tuse\tneed\tlike\tgreat\n",
      "Topic #1: customer\tservice\tissue\tsupport\tcontact\tresponse\tUber\tcompany\tresolve\trefund\n",
      "Topic #2: cancel\tdriver\tcharge\tfee\twait\ttrip\tcancellation\tpick\tcall\trequest\n",
      "Topic #3: walk\tdrop\tpick\tlocation\tpool\tblock\tdestination\taddress\tpoint\tstreet\n",
      "Topic #4: $\tcharge\tprice\tride\tUber\tpay\tcost\tmoney\tdollar\ttake\n",
      "Topic #5: card\tcredit\tpayment\tgift\tmethod\tcash\tuse\tUber\taccount\tdebit\n",
      "Topic #6: minute\twait\taway\ttime\t10\t20\t10 minute\t5\t15\tlate\n",
      "Topic #7: uber\tlyft\tmoney\ttime\tuse\tnt\twork\tneed\tlike\tbad\n",
      "Topic #8: car\tdrive\tseat\tget\tairport\task\tsay\tclean\tsmell\tdriver\n",
      "Topic #9: account\tnumber\tphone\temail\ttry\thelp\tlog\tsign\tapp\tUber\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, top_terms in model.top_topic_terms(\n",
    "    vectorizer.id_to_term, topics=range(n_topics)\n",
    "):\n",
    "    print(f\"Topic #{topic_idx}:\", \"\\t\".join(top_terms))\n",
    "    lst_topics.append(Topic(terms=[str(term) for term in top_terms]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c5ca5778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top documents for each topic\n",
    "for topic_idx, top_docs_idx in model.top_topic_docs(\n",
    "    doc_topic_matrix, weights=True, top_n=20\n",
    "):\n",
    "    #     print(f\"{topic_idx}: {top_docs_idx}\")\n",
    "    lst_topics[topic_idx].top_docs_idx = top_docs_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0502c371",
   "metadata": {},
   "source": [
    "# Exploring Each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2298001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Ngrams, Entities and Noun Chunks for each document in a topic\n",
    "for topic in lst_topics:\n",
    "    l_terms, k_terms = [], []\n",
    "    for doc_idx, weight in topic.top_docs_idx:\n",
    "        doc = corpus[doc_idx]\n",
    "        l_terms.extend(\n",
    "            [\n",
    "                str(term).lower()  # ignore case\n",
    "                for term in extract.terms(doc, ngs=[2, 3], ncs=True, ents=True)\n",
    "            ]\n",
    "        )\n",
    "        k_terms.extend([pair[0] for pair in extract.keyterms.yake(doc, ngrams=[2, 3])])\n",
    "    #     print(len(topic.linguistic_terms))\n",
    "    topic.linguistic_terms = Counter(l_terms).most_common(200)\n",
    "    topic.linguistic_terms, topic.keyterms = get_terms(\n",
    "        topic.linguistic_terms\n",
    "    ), get_terms(topic.keyterms)\n",
    "    topic.keyterms = Counter(k_terms).most_common(200)\n",
    "    # remove duplicates across docs\n",
    "#     print(len(topic.linguistic_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "97e83cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_terms(terms: List[Tuple[str, int]]) -> List[str]:\n",
    "    return [ele[0] for ele in terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eec5b851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['app', 'driver', 'ride', 'time', 'work', 'need', 'Uber', 'use', 'like', 'great', ('Lyft app', 2), ('couple time', 1), ('helpful driver', 1), ('great rating', 1), ('driver time', 1), ('couple thing', 1), ('cancellation tab', 1), ('star rating', 1), ('bad review', 1), ('phone battery', 1)]\n",
      "['customer', 'service', 'issue', 'support', 'response', 'company', 'refund', 'contact', 'Uber', 'resolve', ('customer service', 20), ('customer support', 6), ('service number', 4), ('customer service number', 4), ('phone number', 3), ('terrible customer', 2), ('poor customer', 2), ('poor customer service', 2), ('horrible customer', 2), ('horrible customer service', 2)]\n",
      "['driver', 'fee', 'trip', 'request', 'cancel', 'charge', 'wait', 'cancellation', 'pick', 'call', ('cancellation fee', 12), ('Uber driver', 4), ('new driver', 2), ('phone number', 2), ('second driver', 2), ('long time', 2), ('Ciudad Juarez', 1), ('El Paso', 1), ('cancel fee', 1), ('bad thing', 1)]\n",
      "['drop', 'pick', 'location', 'pool', 'block', 'destination', 'address', 'point', 'street', 'walk', ('Uber pool', 5), ('uber pool', 4), ('Uber driver', 3), ('multiple occasion', 3), ('new pool', 2), ('different location', 2), ('recent update', 2), ('min walk', 2), ('new Uber', 1), ('place close', 1)]\n",
      "['price', 'ride', 'cost', 'money', '$', 'charge', 'Uber', 'pay', 'dollar', 'take', ('dollar ride', 2), ('customer service', 2), ('surge price', 1), ('rare occasion', 1), ('basic service', 1), ('monthly metrocard', 1), ('driver scarce', 1), ('exact distance', 1), ('price gauge', 1), ('minute ride', 1)]\n",
      "['card', 'payment', 'use', 'account', 'credit', 'gift', 'method', 'cash', 'Uber', 'debit', ('credit card', 13), ('gift card', 10), ('payment method', 6), ('debit card', 4), ('card balance', 3), ('gift card balance', 3), ('Uber gift', 3), ('card info', 2), ('personal card', 2), ('card information', 2)]\n",
      "['minute', 'time', '10', 'wait', 'away', '20', '10 minute', '5', '15', 'late', ('wait time', 5), ('minute wait', 3), ('arrival time', 2), ('second driver', 2), ('affordable option', 1), ('gas station', 1), ('drunk driving', 1), ('App Store', 1), ('minute delay', 1), ('drive cancel', 1)]\n",
      "['uber', 'lyft', 'money', 'time', 'work', 'use', 'nt', 'need', 'like', 'bad', ('uber driver', 6), ('customer service', 3), ('time uber', 2), ('bad experience', 2), ('uber ride', 2), ('extra charge', 1), ('charge fee', 1), ('new uber', 1), ('extra charge fee', 1), ('timely fashion', 1)]\n",
      "['car', 'airport', 'smell', 'driver', 'drive', 'seat', 'get', 'ask', 'say', 'clean', ('car seat', 2), ('correct car', 2), ('car color', 2), ('right car', 2), ('Uber driver', 2), ('Uber app', 1), ('car ride', 1), ('nice woman', 1), ('safe drive', 1), ('wonderful employee', 1)]\n",
      "['account', 'number', 'email', 'try', 'help', 'app', 'phone', 'log', 'sign', 'Uber', ('phone number', 16), ('Uber account', 6), ('new account', 5), ('email address', 4), ('new number', 4), ('customer service', 3), ('old number', 2), ('credit card', 2), ('new phone', 2), ('service number', 2)]\n"
     ]
    }
   ],
   "source": [
    "# Finding a Title for Each Topic\n",
    "for topic in lst_topics:\n",
    "    combined_list = topic.terms + topic.keyterms + topic.linguistic_terms\n",
    "    combined_list = Counter(combined_list).most_common(20)\n",
    "    print(get_terms(combined_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in lst_topics:\n",
    "    print(topic.title, topic.terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1338a",
   "metadata": {},
   "source": [
    "# Labels\n",
    "\n",
    "Happy with the app [Skipped]\n",
    "\n",
    "|Index|Label Title| Label Description|\n",
    "|---|:---|:---|\n",
    "|1|payment|Payment Methods|\n",
    "|2|cancel_fees|Cancellation Fee|\n",
    "|3|price|Price|\n",
    "|4|pickup|Pickup|\n",
    "|5|pool|Pool|\n",
    "|6|support|Customer Support|\n",
    "|7|advance_ride|Advance Ride Booking|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
