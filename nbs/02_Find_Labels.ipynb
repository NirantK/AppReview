{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a369a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import textacy\n",
    "from pydantic import BaseModel\n",
    "from spacy.lang.en import English\n",
    "from textacy import extract\n",
    "from textacy.representations.vectorizers import Vectorizer\n",
    "from textacy.similarity.tokens import jaccard\n",
    "from textacy.tm import TopicModel\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9364bba0",
   "metadata": {},
   "source": [
    "The main contribution here is a programmatic way to find labels for topic models, and then classify documents into them -- but while still retaining some degree of human intervention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adc034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"../data\").resolve()\n",
    "assert data_dir.exists()\n",
    "data_dir.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2d73f",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "Here we will explore App Reviews for just one app: Uber (Passenger/Cab, not the Driver). The additional data to reproduce this for other clients is left as an exercise for you.\n",
    "\n",
    "But to get an overview of all of them, we combine them into a larger single text string and explore them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb4aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = data_dir / \"Uber_us_app_store_reviews.json\"; assert file_path.exists()\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = pd.read_json(f)\n",
    "    reviews = raw_data[\"review\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de20c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time reviews = [rev.strip() for rev in reviews]\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0e0d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time corpus = textacy.Corpus(\"en_core_web_sm\", data=reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68e6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.n_docs, corpus.n_sents, corpus.n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aab5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = corpus.word_counts(by=\"lemma_\", filter_stops= True, filter_nums=True, filter_punct=True)\n",
    "sorted(word_counts.items(), key=lambda x: x[1], reverse=True)[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d669fb39",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- [ ] Topic Modeling\n",
    "- [ ] Noun Chunks\n",
    "- [ ] Verb\n",
    "- [ ] SVO Triples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4daa05",
   "metadata": {},
   "source": [
    "## Topic Modeling with Textacy via Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d44e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_docs = ((term.lemma_ for term in textacy.extract.terms(doc, ngs=1, ents=True)) for doc in corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d88166",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = Vectorizer(tf_type=\"linear\", idf_type=\"smooth\", norm=\"l2\",min_df=3, max_df=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27d978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = vectorizer.fit_transform(tokenized_docs)\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7f72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75dee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TopicModel(\"nmf\", n_topics=n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1102457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919078d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topic_matrix = model.transform(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d7eac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Topic(BaseModel):\n",
    "    title: Optional[str]\n",
    "    terms: Optional[List[str]]\n",
    "    top_docs_idx: Optional[List[int]]\n",
    "    keyterms: Optional[List[str]] = []\n",
    "    linguistic_terms: Optional[List[str]] = []\n",
    "    \n",
    "lst_topics = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd9e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic_idx, top_terms in model.top_topic_terms(vectorizer.id_to_term, topics=range(n_topics)):\n",
    "    print(f\"Topic #{topic_idx}:\", \"\\t\".join(top_terms))\n",
    "    lst_topics.append(Topic(terms=[str(term) for term in top_terms]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ca5778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top documents for eah topic\n",
    "for topic_idx, top_docs_idx in model.top_topic_docs(doc_topic_matrix, weights=True, top_n=20):\n",
    "#     print(f\"{topic_idx}: {top_docs_idx}\")\n",
    "    lst_topics[topic_idx].top_docs_idx = top_docs_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a8727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noun Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74843a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2298001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in lst_topics:\n",
    "    for doc_idx, weight in topic.top_docs_idx:\n",
    "        doc = corpus[doc_idx]\n",
    "        topic.linguistic_terms.extend([str(term) for term in extract.terms(doc, ngs=[3, 4, 5], ncs=True, ents=True)])\n",
    "        topic.keyterms.extend([pair[0] for pair in extract.keyterms.yake(doc, ngrams = [3])])\n",
    "#     print(len(topic.linguistic_terms))\n",
    "    topic.linguistic_terms = list(set(topic.linguistic_terms)) # remove duplicates across docs\n",
    "#     print(len(topic.linguistic_terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e83cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic.linguistic_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832022cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in tqdm(lst_topics):\n",
    "    max_jaccard = 0\n",
    "    for candidate in topic.linguistic_terms:\n",
    "        jaccard_score = jaccard(candidate.split(), topic.terms)\n",
    "        if jaccard_score > max_jaccard:\n",
    "            topic.title = candidate\n",
    "            max_jaccard = jaccard(candidate.split(), topic.terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d77cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in lst_topics:\n",
    "    print(topic.title, topic.terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1338a",
   "metadata": {},
   "source": [
    "# Labels\n",
    "\n",
    "Happy with the app [Skipped]\n",
    "\n",
    "|Index|Label Title| Label Description|\n",
    "|---|:---|:---|\n",
    "|1|payment|Payment Methods|\n",
    "|2|cancel_fees|Cancellation Fee|\n",
    "|3|price|Price|\n",
    "|4|pickup|Pickup|\n",
    "|5|pool|Pool|\n",
    "|6|support|Customer Support|\n",
    "|7|advance_ride|Advance Ride Booking|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
