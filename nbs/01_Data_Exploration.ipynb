{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fc4a369a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import textacy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "Path.ls = lambda x: list(x.iterdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2adc034e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/nirant/AppReview/data/Clubhouse_us_app_store_reviews.json'),\n",
       " PosixPath('/home/nirant/AppReview/data/frequency_count.json'),\n",
       " PosixPath('/home/nirant/AppReview/data/Moj_us_app_store_reviews.json'),\n",
       " PosixPath('/home/nirant/AppReview/data/Uber_us_app_store_reviews.json'),\n",
       " PosixPath('/home/nirant/AppReview/data/com.ubercab_us_play_store_reviews.json'),\n",
       " PosixPath('/home/nirant/AppReview/data/Netflix_us_app_store_reviews.json'),\n",
       " PosixPath('/home/nirant/AppReview/data/Moj_in_app_store_reviews.json'),\n",
       " PosixPath('/home/nirant/AppReview/data/IndiaGold_in_app_store_reviews.json')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = Path(\"../data\").resolve()\n",
    "assert data_dir.exists()\n",
    "data_dir.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc2d73f",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "Here we will explore App Reviews for just one app: Uber (Passenger/Cab, not the Driver). The additional data to reproduce this for other clients is left as an exercise for you.\n",
    "\n",
    "But to get an overview of all of them, we combine them into a larger single text string and explore them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb4aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = data_dir / \"Uber_us_app_store_reviews.json\"; assert file_path.exists()\n",
    "with file_path.open(\"r\") as f:\n",
    "    raw_data = pd.read_json(f)\n",
    "    reviews = \" \".join(raw_data[\"review\"].to_list())\n",
    "# print(type(reviews), reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f56f09c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = []\n",
    "files = [x for x in data_dir.ls() if \"app\" in x.name]\n",
    "for file_path in files:\n",
    "    try:\n",
    "        with file_path.open(\"r\") as f:\n",
    "            file_data = pd.read_json(f)\n",
    "            reviews += file_data[\"review\"].to_list()\n",
    "    except ValueError as e:\n",
    "        print(f\"Value Error with {file.name}\")\n",
    "    except KeyError as ke:\n",
    "        print(f\"Key error with {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb72ea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9762e280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6049"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afceae11",
   "metadata": {},
   "source": [
    "Since we've limited memory on this machine, disabling memory heavy parts of the pipeline. If we combine all reviews into one large chunk -- this is what we get. \n",
    "\n",
    "```python-traceback\n",
    "ValueError: [E088] Text of length 2737687 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2c32e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_trf\", disable=[\"parser\", \"ner\"])\n",
    "# nlp.max_length = len(reviews) + 1\n",
    "# nlp(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446424d",
   "metadata": {},
   "source": [
    "Even after disabling the components, it will take extremely long durations and a lot of memory to process our relatively \"small data\". The solution? Batch your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cdaaa007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1de57abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.36 ms, sys: 0 ns, total: 1.36 ms\n",
      "Wall time: 1.37 ms\n"
     ]
    }
   ],
   "source": [
    "%time reviews = [rev.strip() for rev in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "72e244a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "c91c6bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "\n",
    "class TextSummary:\n",
    "    def __init__(self, records: List[str]):\n",
    "        self.token_summary = self.make_summary(records)\n",
    "        self.token_stats = self.get_corpora_stats(self.token_summary)\n",
    "\n",
    "    def make_summary(self, records: List[str]):\n",
    "        token_vocab = []\n",
    "        for doc in tokenizer.pipe(records):\n",
    "            token_vocab.append(Counter([str(x) for x in doc]))\n",
    "\n",
    "        \"\"\"Get a Count Distribution for the entire Corpora\"\"\"\n",
    "        count_summary = token_vocab[0]\n",
    "        for record in tqdm(token_vocab[1:]):\n",
    "            count_summary += record\n",
    "\n",
    "        return count_summary\n",
    "\n",
    "    def get_corpora_stats(self, summary: List[Counter]):\n",
    "        self.vocab = list(summary.keys())\n",
    "        self.vocab_sz = len(self.vocab)\n",
    "        self.size = sum(summary[key] for key in summary.keys())\n",
    "        return {\"size\": self.size, \"vocab_sz\": self.vocab_sz, \"vocab\": self.vocab}\n",
    "\n",
    "\n",
    "#         count, uniques = 0, []\n",
    "#         for record in summary:\n",
    "#             record_sz = sum([record[key] for key in record.keys()])\n",
    "#             uniques.extend(list(record.keys()))\n",
    "#             count += record_sz\n",
    "\n",
    "#         \"\"\"Get Unique Words and Their Count\"\"\"\n",
    "#         uniques = set(uniques)\n",
    "#         unique_count = len(uniques)\n",
    "#         return count, uniques, unique_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "66fb8b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7e9cdec5b84c08b7972b06b4abe0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tsm = TextSummary(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "50c9afc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 22666),\n",
       " ('I', 21739),\n",
       " ('the', 18346),\n",
       " ('to', 17547),\n",
       " ('and', 14836),\n",
       " (',', 12667),\n",
       " ('a', 10802),\n",
       " ('it', 9598),\n",
       " ('is', 6861),\n",
       " ('that', 6747),\n",
       " ('of', 6681),\n",
       " ('my', 5937),\n",
       " ('you', 5886),\n",
       " ('for', 5472),\n",
       " ('!', 5173),\n",
       " ('have', 4827),\n",
       " ('n’t', 4820),\n",
       " ('on', 4607),\n",
       " ('in', 4589),\n",
       " ('app', 4377),\n",
       " ('but', 4058),\n",
       " ('this', 3964),\n",
       " ('was', 3815),\n",
       " ('me', 3523),\n",
       " ('’s', 3475)]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsm.token_summary.most_common(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
